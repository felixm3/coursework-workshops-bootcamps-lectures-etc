---
title: "*K*-Nearest Neighbors (*k*NN) Classification & Cross-Validation"
output: rmarkdown::github_document
date: '2022-08-08'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.retina = 2)
```

## Iris

PROBLEM STATEMENT: for the *k*NN classifier, compare the 5-fold, 10-fold, and leave-one-out cross-validation error rates for *k* = 1, ..., 50 on the classic `Iris` dataset

### Load the Data

```{r}

# import iris data
iris.data <- read.csv(
  "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data", 
  header = FALSE, 
  col.names = c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species"))


```

### Look at the Data

```{r}

head(iris.data)

iris.data[5] <- factor(iris.data[[5]])

str(iris.data)

summary(iris.data)

```

```{r}

pairs(~ ., data = iris.data[-5], 
      col = factor(iris.data[[5]])) 
# this would be clearer if used `Species` instead of `5` in the command

```

### Fit and Validate the *k*NN Model

I used the `caret` package for model fitting and cross-validation

```{r}

# load caret package
library(caret)

```

I tested 5-fold, 10-fold, and leave-one-out cross-validation (CV)

```{r}

# 5-fold, 10-fold, and leave-one-out (LOO) CV
numFolds <- c(5, 10, 150)

# list to store the different fits
fitList <- vector(mode = "list", length = length(numFolds))
names(fitList) <- numFolds

for(i in 1:length(numFolds)){
  trControl <- trainControl(method = "cv",
                            number = numFolds[i])

# vary the number of nearest neighbors k from 1 to 50
set.seed(1250)
fitList[[i]] <- train(Species ~ .,
                        method = "knn",
                        tuneGrid = expand.grid(k = 1:50),
                        trControl = trControl,
                        metric = "Accuracy",
                        data = iris.data)
}
fitList

```

### Analyze Performance Metrics

Plot the results showing the comparison in performance

```{r}

plot(1:50, 1-fitList[[1]]$results$Accuracy,
     type = "o", col = "red",
     xlab = "k (number of nearest neighbors used)", 
     ylab = "Cross-Validation Error Rate", 
     ylim = c(0.01, 0.091))
points(1:50, 1-fitList[[2]]$results$Accuracy, type = "o", col = "blue")
points(1:50, 1-fitList[[3]]$results$Accuracy, type = "o", col = "black")
legend("topleft", legend = c("5-fold CV", "10-fold CV", "LOOCV"),
       col = c("red", "blue", "black"), lty = 1)

```

## *k*NN classifer with different distance metrics: USPS digits

PROBLEM STATEMENT: for the *k*NN classifier, compare the test error rates for three different distance metrics (Euclidean, Manhattan, cosine) on the [USPS handwritten zip code digits dataset](https://hastie.su.domains/ElemStatLearn/data.html) 

Load the data

```{r}

library(tidyverse)
usps_train <- read_delim(
  "https://hastie.su.domains/ElemStatLearn/datasets/zip.train.gz", 
  delim = " ", col_names = FALSE)
usps_test <- read_delim(
  "https://hastie.su.domains/ElemStatLearn/datasets/zip.test.gz", 
  delim = " ", col_names = FALSE)

```

Look at the data.

[Each row is](https://hastie.su.domains/ElemStatLearn/datasets/zip.info.txt) the 256 normalized grayscale values of the 16 x 16 pixel image of a digit.

The first column contains the identity (label) of the digit. The last column of `usps_train` is all NA.

```{r}

# str(usps_train)

usps_train <- as.matrix(usps_train)
usps_test <- as.matrix(usps_test)

# first column contains labels
usps_train_labels <- usps_train[, 1]
usps_test_labels <- usps_test[, 1]

# drop first column of usps_train and usps_test
usps_train <- usps_train[, -1]
usps_test <- usps_test[, -1]

# last column of usps_train is all NA
# summary(usps_train[, 257])

# drop last column (all NA) of usps_train
usps_train <- usps_train[, -257]

# head(usps_train)
# head(usps_test)

str(usps_train)
str(usps_test)

```
Convert labels to factors

```{r}

usps_train_labels <- factor(usps_train_labels)
usps_test_labels <- factor(usps_test_labels)

str(usps_train_labels)
str(usps_test_labels)

```

Define functions that calculate distance (Euclidean, Manhattan, cosine) between two vectors. These will be used to calculate the `n x m` distances between each of the `n` test points and all the `m` train points 

```{r define distance functions}

euclidean_dist <- function(x1, x2){
  # calculates Euclidean distance between vectors x1 and x2
  return(sqrt(sum((x1 - x2) ^ 2)))
}
manhattan_dist <- function(x1, x2){
  # calculates Manhattan distance between vectors x1 and x2
  return(sum(abs(x1 - x2)))
}
cosine_dist <- function(x1, x2){
  # calculates Cosine distance between vectors x1 and x2
  # note, however, that the cosine distance is not a proper distance metric as it does not have the triangle inequality property
  # ?`%*%`
  return(1 - ((x1 %*% x2)/(sqrt(x1 %*% x1) * sqrt(x2 %*% x2))))
}

```

Calculate distance matrices

For the three 'distances' (Euclidean, Manhattan, cosine), we create a matrix to store distances between each test point and all the train points. Each row of the distance matrix is a test point, each column is a train point, and the element (i, j) is the distance between test point i and train point j. This matrix will be used to figure out each test point's nearest neighbors.


```{r initialize distance matrices}

distances_Euclidean <- matrix(0, nrow = nrow(usps_test), ncol = nrow(usps_train))
distances_Manhattan <- matrix(0, nrow = nrow(usps_test), ncol = nrow(usps_train))
distances_Cosine <- matrix(0, nrow = nrow(usps_test), ncol = nrow(usps_train))

```

Populate the distance matrices


kNN usually requires scaling before calculating distances to ensure all features are on the same footing. But since this data has already been scaled (all values between -1 and 1), this step was skipped.

```{r}

system.time({
  for(i in 1:nrow(usps_test)){
    for(j in 1:nrow(usps_train)){
      distances_Euclidean[i, j] <- euclidean_dist(usps_test[i, ], usps_train[j, ])
      distances_Manhattan[i, j] <- manhattan_dist(usps_test[i, ], usps_train[j, ])
      distances_Cosine[i, j] <- cosine_dist(usps_test[i, ], usps_train[j, ])
    }
  }
})

```

took ~ 630s to create the matrices

- how to compare 2 matrices?
  - element-wise correlation

```{r}

# create a matrix with k columns and nrow(usps_test) rows to store predictions for every test point using 1 to k neighbors
usps_test_pred_Euclidean <- matrix(0, nrow = nrow(usps_test), ncol = 10)
# for each test point i
for(i in 1:nrow(usps_test)){
  # cbind train set, train labels and dist[i, ] and sort by distance
  trSetLabDist <- data.frame(usps_train, usps_train_labels, distances_Euclidean[i, ])
  trSetLabDist <- trSetLabDist[order(distances_Euclidean[i, ]), ] # sort by distance
  # head(trSetLabDist[ , 250:258], 10)
  # make prediction for test point i for k = 1 to k = 10 (10 predictions per test point)
  z <- integer(10) # vector to store counts of ten classes
  z_labels <- levels(usps_train_labels)
  for(k in 1:10){ # predictions for k = 1 to k = 10
    for(m in 1:10){ # count how many times each class appears in the k neighbors
      z[m] <- sum(trSetLabDist[1:k, "usps_train_labels"] == z_labels[m])
    }
    usps_test_pred_Euclidean[i, k] <- sample(z_labels[which(z == max(z))], size = 1) # assign to class with highest z; in case of tie, pick randomly
  }
} #74.241 sec

```
Now do for Manhattan and cosine distance

```{r}

# create a matrix with k columns and nrow(usps_test) rows to store predictions for every test point using 1 to k neighbors
usps_test_pred_Manhattan <- matrix(0, nrow = nrow(usps_test), ncol = 10)
# for each test point i
for(i in 1:nrow(usps_test)){
  # cbind train set, train labels and dist[i, ] and sort by distance
  trSetLabDist <- data.frame(usps_train, usps_train_labels, distances_Manhattan[i, ])
  trSetLabDist <- trSetLabDist[order(distances_Manhattan[i, ]), ] # sort by distance
  # head(trSetLabDist[ , 250:258], 10)
  # make prediction for test point i for k = 1 to k = 10 (10 predictions per test point)
  z <- integer(10) # vector to store counts of ten classes
  z_labels <- levels(usps_train_labels)
  for(k in 1:10){ # predictions for k = 1 to k = 10
    for(m in 1:10){ # count how many times each class appears in the k neighbors
      z[m] <- sum(trSetLabDist[1:k, "usps_train_labels"] == z_labels[m])
    }
    usps_test_pred_Manhattan[i, k] <- sample(z_labels[which(z == max(z))], size = 1) # assign to class with highest z; in case of tie, pick randomly
  }
}


# create a matrix with k columns and nrow(usps_test) rows to store predictions for every test point using 1 to k neighbors
usps_test_pred_Cosine <- matrix(0, nrow = nrow(usps_test), ncol = 10)
# for each test point i
for(i in 1:nrow(usps_test)){
  # cbind train set, train labels and dist[i, ] and sort by distance
  trSetLabDist <- data.frame(usps_train, usps_train_labels, distances_Cosine[i, ])
  trSetLabDist <- trSetLabDist[order(distances_Cosine[i, ]), ] # sort by distance
  # head(trSetLabDist[ , 250:258], 10)
  # make prediction for test point i for k = 1 to k = 10 (10 predictions per test point)
  z <- integer(10) # vector to store counts of ten classes
  z_labels <- levels(usps_train_labels)
  for(k in 1:10){ # predictions for k = 1 to k = 10
    for(m in 1:10){ # count how many times each class appears in the k neighbors
      z[m] <- sum(trSetLabDist[1:k, "usps_train_labels"] == z_labels[m])
    }
    usps_test_pred_Cosine[i, k] <- sample(z_labels[which(z == max(z))], size = 1) # assign to class with highest z; in case of tie, pick randomly
  }
}
```

Calculate error rates for the three distances

```{r}

library(caret) # for function confusionMatrix

errorRate_Euclidean <- numeric(10)
for(i in 1:10){
  errorRate_Euclidean[i] <- 1 - confusionMatrix(as.factor(usps_test_pred_Euclidean[, i]), usps_test_labels)$overall["Accuracy"]
}

errorRate_Manhattan <- numeric(10)
for(i in 1:10){
  errorRate_Manhattan[i] <- 1 - confusionMatrix(as.factor(usps_test_pred_Manhattan[, i]), usps_test_labels)$overall["Accuracy"]
}

errorRate_Cosine <- numeric(10)
for(i in 1:10){
  errorRate_Cosine[i] <- 1 - confusionMatrix(as.factor(usps_test_pred_Cosine[, i]), usps_test_labels)$overall["Accuracy"]
}

```

Plot the error rates


```{r}

plot(1:10, errorRate_Euclidean,
     type = "o", col = "red",
     xlab = "k (number of nearest neighbors used)", ylab = "Misclassification Error Rate",
     ylim = c(0.05, 0.08))
points(1:10, errorRate_Manhattan, type = "o", col = "blue")
points(1:10, errorRate_Cosine, type = "o", col = "black")
legend("top", legend = c("Euclidean", "Manhattan", "Cosine"),
       col = c("red", "blue", "black"), lty = 1)

cbind(errorRate_Cosine, errorRate_Euclidean, errorRate_Manhattan)

```

*how do I add error bars to above graph?*

## weighted *k*NN USPS digits

PROBLEM STATEMENT: apply the weighted *k*NN classifier (Euclidean distance, weights: inverse, squared inverse, and linear) on the [USPS handwritten zip code digits dataset](https://hastie.su.domains/ElemStatLearn/data.html) and compare test errors. 

Load the data

```{r}

library(tidyverse)
usps_train <- read_delim(
  "https://hastie.su.domains/ElemStatLearn/datasets/zip.train.gz", 
  delim = " ", col_names = FALSE)
usps_test <- read_delim(
  "https://hastie.su.domains/ElemStatLearn/datasets/zip.test.gz", 
  delim = " ", col_names = FALSE)

# str(usps_train)

usps_train <- as.matrix(usps_train)
usps_test <- as.matrix(usps_test)

# first column contains labels
usps_train_labels <- usps_train[, 1]
usps_test_labels <- usps_test[, 1]

# drop first column of usps_train and usps_test
usps_train <- usps_train[, -1]
usps_test <- usps_test[, -1]

# drop last column (all NA) of usps_train
usps_train <- usps_train[, -257]

# convert labels to factors
usps_train_labels <- factor(usps_train_labels)
usps_test_labels <- factor(usps_test_labels)

str(usps_train_labels)
str(usps_test_labels)

```

Is it faster to apply a function to a matrix directly `foo(M)` or to use `apply` to do it `apply(M, c(1, 2), foo)`?

```{r}

library(microbenchmark)

inv <- function(x){1/x}

M <- matrix(1:1000000, nrow = 1000)

microbenchmark(inv(M), 
               apply(M, 1, inv))


```
On average, using the function directly on the matrix is ~7X faster than using `apply`

Define functions for the three weighting methods

```{r}

inv <- function(x, k = 1){k/x}

inv_sqr <- function(x, k = 1){k/x^2}

lin <- function(x, k = 1){k - x}

plot(inv, 0, 10)
curve(inv_sqr, add = TRUE, col = "red")
curve(lin, add = TRUE, col = "blue")

```

Create Euclidean distance matrix to store Euclidean distances between each test point and all the train points. Each row of the distance matrix is a test point, each column is a train point, and the matrix element (i, j) is the distance between test point i and train point j. This matrix will be used to figure out each test point's nearest neighbors.

```{r}

euclidean_dist <- function(x1, x2){
  # calculates Euclidean distance between vectors x1 and x2
  return(sqrt(sum((x1 - x2) ^ 2)))
}

# initialize matrix
distances_Euclidean <- matrix(0, nrow = nrow(usps_test), ncol = nrow(usps_train))

# populate matrix
ptm <- proc.time()
for(i in 1:nrow(usps_test)){
  for(j in 1:nrow(usps_train)){
    distances_Euclidean[i, j] <- euclidean_dist(usps_test[i, ], usps_train[j, ])
  }
}
proc.time() - ptm

# ~3.5 minutes
```

kNN with inverse weights

```{r}

# create a matrix with k columns and nrow(usps_test) rows to store predictions for every test point using 1 to k neighbors
# element [i, k] is the prediction of test point i using k neighbors
usps_test_pred_Euclidean_inv <- matrix(0, nrow = nrow(usps_test), ncol = 10)
# for each test point i
for(i in 1:nrow(usps_test)){
  # cbind train labels and dist[i, ] and sort by distance
  trSetLabDist <- data.frame(usps_train_labels, distances_Euclidean[i, ])
  trSetLabDist <- trSetLabDist[order(distances_Euclidean[i, ]), ] # sort by distance
  max_dist <- max(trSetLabDist[, 2])
  # make prediction for test point i for k = 1 to k = 10 (10 predictions per test point)
  z <- integer(10) # vector to store *WEIGHTED* counts of ten classes
  z_labels <- levels(usps_test_labels)
  for(k in 1:10){ # predictions for k = 1 to k = 10
    for(m in 1:k){ # loop through the k neighbors and add the weight to z
      z[which(z_labels == trSetLabDist[m, 1])] <- 
        z[which(z_labels == trSetLabDist[m, 1])] + inv(trSetLabDist[m, 2], k = max_dist)
    }
    usps_test_pred_Euclidean_inv[i, k] <- sample(z_labels[which(z == max(z))], size = 1) # assign to class with highest z; in case of tie, pick randomly
  }
} 

head(cbind(usps_test_labels, as.factor(usps_test_pred_Euclidean_inv)))


```

kNN with squared inverse weight

```{r}

# create a matrix with k columns and nrow(usps_test) rows to store predictions for every test point using 1 to k neighbors
# element [i, k] is the prediction of test point i using k neighbors
usps_test_pred_Euclidean_inv_sqr <- matrix(0, nrow = nrow(usps_test), ncol = 10)
# for each test point i
for(i in 1:nrow(usps_test)){
  # cbind train labels and dist[i, ] and sort by distance
  trSetLabDist <- data.frame(usps_train_labels, distances_Euclidean[i, ])
  trSetLabDist <- trSetLabDist[order(distances_Euclidean[i, ]), ] # sort by distance
  max_dist <- max(trSetLabDist[, 2])
  # make prediction for test point i for k = 1 to k = 10 (10 predictions per test point)
  z <- integer(10) # vector to store *WEIGHTED* counts of ten classes
  z_labels <- levels(usps_test_labels)
  for(k in 1:10){ # predictions for k = 1 to k = 10
    for(m in 1:k){ # loop through the k neighbors and add the weight to z
      z[which(z_labels == trSetLabDist[m, 1])] <- 
        z[which(z_labels == trSetLabDist[m, 1])] + inv_sqr(trSetLabDist[m, 2], k = max_dist)
    }
    usps_test_pred_Euclidean_inv_sqr[i, k] <- sample(z_labels[which(z == max(z))], size = 1) # assign to class with highest z; in case of tie, pick randomly
  }
} 

head(cbind(usps_test_labels, as.factor(usps_test_pred_Euclidean_inv_sqr)))


```


kNN with linear weight

```{r}

# create a matrix with k columns and nrow(usps_test) rows to store predictions for every test point using 1 to k neighbors
# element [i, k] is the prediction of test point i using k neighbors
usps_test_pred_Euclidean_lin <- matrix(0, nrow = nrow(usps_test), ncol = 10)
# for each test point i
for(i in 1:nrow(usps_test)){
  # cbind train labels and dist[i, ] and sort by distance
  trSetLabDist <- data.frame(usps_train_labels, distances_Euclidean[i, ])
  trSetLabDist <- trSetLabDist[order(distances_Euclidean[i, ]), ] # sort by distance
  max_dist <- max(trSetLabDist[, 2])
  # make prediction for test point i for k = 1 to k = 10 (10 predictions per test point)
  z <- integer(10) # vector to store *WEIGHTED* counts of ten classes
  z_labels <- levels(usps_test_labels)
  for(k in 1:10){ # predictions for k = 1 to k = 10
    for(m in 1:k){ # loop through the k neighbors and add the weight to z
      z[which(z_labels == trSetLabDist[m, 1])] <- 
        z[which(z_labels == trSetLabDist[m, 1])] + lin(trSetLabDist[m, 2], k = max_dist)
    }
    usps_test_pred_Euclidean_lin[i, k] <- sample(z_labels[which(z == max(z))], size = 1) # assign to class with highest z; in case of tie, pick randomly
  }
} 

head(cbind(usps_test_labels, as.factor(usps_test_pred_Euclidean_lin)))


```

Calculate error rates for the three weighting schemes

```{r}

library(caret) # for function confusionMatrix

errorRate_Euclidean_inv <- numeric(10)
for(i in 1:10){
  errorRate_Euclidean_inv[i] <- 1 - confusionMatrix(as.factor(usps_test_pred_Euclidean_inv[, i]), usps_test_labels)$overall["Accuracy"]
}

errorRate_Euclidean_inv_sqr <- numeric(10)
for(i in 1:10){
  errorRate_Euclidean_inv_sqr[i] <- 1 - confusionMatrix(as.factor(usps_test_pred_Euclidean_inv_sqr[, i]), usps_test_labels)$overall["Accuracy"]
}

errorRate_Euclidean_lin <- numeric(10)
for(i in 1:10){
  errorRate_Euclidean_lin[i] <- 1 - confusionMatrix(as.factor(usps_test_pred_Euclidean_lin[, i]), usps_test_labels)$overall["Accuracy"]
}

```

Plot the error rates


```{r}

plot(1:10, errorRate_Euclidean_inv,
     type = "o", col = "red",
     xlab = "k (number of nearest neighbors used)", ylab = "Misclassification Error Rate",
     ylim = c(0.052, 0.057))
points(1:10, errorRate_Euclidean_inv_sqr, type = "o", col = "blue")
points(1:10, errorRate_Euclidean_lin, type = "o", col = "black")
legend("top", legend = c("inverse", "squared inverse", "linear"),
       col = c("red", "blue", "black"), lty = 1)

cbind(errorRate_Euclidean_inv, errorRate_Euclidean_inv_sqr, errorRate_Euclidean_lin)

```

---

## nearest local centroid USPS digits

PROBLEM STATEMENT: Apply the nearest local centroid classifier to the USPS digits data with different values of *k*.
Plot the test error curve and interpret your results.
What is the confusion matrix corresponding to the optimal *k*?


Strategy
- set initial centroid for all labels to max_dist
- update centroids for labels within *k* neighbors
- assign label as which.min(centroids)

Load the data

```{r}

rm(list = ls())
gc()

library(tidyverse)
usps_train <- read_delim(
  "https://hastie.su.domains/ElemStatLearn/datasets/zip.train.gz", 
  delim = " ", col_names = FALSE)
usps_test <- read_delim(
  "https://hastie.su.domains/ElemStatLearn/datasets/zip.test.gz", 
  delim = " ", col_names = FALSE)

# str(usps_train)

usps_train <- as.matrix(usps_train)
usps_test <- as.matrix(usps_test)

# first column contains labels
usps_train_labels <- usps_train[, 1]
usps_test_labels <- usps_test[, 1]

# drop first column of usps_train and usps_test
usps_train <- usps_train[, -1]
usps_test <- usps_test[, -1]

# drop last column (all NA) of usps_train
usps_train <- usps_train[, -257]

# convert labels to factors
usps_train_labels <- factor(usps_train_labels)
usps_test_labels <- factor(usps_test_labels)

str(usps_train_labels)
str(usps_test_labels)

```

Create Euclidean distance matrix to store Euclidean distances between each test point and all the train points. Each row of the distance matrix is a test point, each column is a train point, and the matrix element (i, j) is the distance between test point i and train point j. This matrix will be used to figure out each test point's nearest neighbors.

```{r}

euclidean_dist <- function(x1, x2){
  # calculates Euclidean distance between vectors x1 and x2
  return(sqrt(sum((x1 - x2) ^ 2)))
}

# initialize matrix
distances_Euclidean <- matrix(0, nrow = nrow(usps_test), ncol = nrow(usps_train))

# populate matrix
ptm <- proc.time()
for(i in 1:nrow(usps_test)){
  for(j in 1:nrow(usps_train)){
    distances_Euclidean[i, j] <- euclidean_dist(usps_test[i, ], usps_train[j, ])
  }
}
proc.time() - ptm

# ~3.5 minutes
```

calculate local centroids and assign test point to nearest local centroid

```{r}

# create a matrix with k columns and nrow(usps_test) rows to store predictions for every test point using 1 to k neighbors
# element [i, k] is the prediction of test point i using k neighbors

max_k <- 40
usps_test_pred_Euclidean_cent <- matrix(0, nrow = nrow(usps_test), ncol = max_k)
# for each test point i
for(i in 1:nrow(usps_test)){
  # i <- 4
  # cbind train labels and dist[i, ] and sort by distance
  trSetLabDist <- data.frame(usps_train_labels, distances_Euclidean[i, ])
  trSetLabDist <- trSetLabDist[order(distances_Euclidean[i, ]), ] # sort by distance
  max_dist <- max(trSetLabDist[, 2])
  # make prediction for test point i for k = 1 to k = max_k (max_k predictions per test point)
  z <- rep(max_dist, 10) # vector to store local centroids of ten classes
  z_labels <- levels(usps_test_labels)
  for(k in 1:max_k){ # predictions for k = 1 to k = max_k
    # loop through the k neighbors calculating centroids & dist to test point i

    neighbors <- data.frame(usps_train_labels, 
                              usps_train)[as.integer(rownames(trSetLabDist[1:k, ])), ]
    # neighbors[, 1:12]
    
    for (lbl in unique(neighbors[, 'usps_train_labels'])){
      #print(lbl)
      #print(neighbors[neighbors$usps_train_labels == lbl, 1:12])
      #neighbors[neighbors$usps_train_labels == lbl, -1]
      
      # group neighbors by label
      lbl_neighbors <- neighbors[neighbors$usps_train_labels == lbl, -1]
      #print(lbl_neighbors[, 1:5])
      
      # calculate local centroid for each label
      lbl_centroid <- colSums(lbl_neighbors)/nrow(lbl_neighbors)
      #print(lbl_centroid[1:5])
      
      # calculate distance from i to each centroid
      z[which(z_labels == lbl)] <- euclidean_dist(usps_test[i, ], lbl_centroid)
      #print(z)
    }

    usps_test_pred_Euclidean_cent[i, k] <- sample(z_labels[which(z == min(z))], size = 1) # assign to class with smallest z i.e. nearest local centroid; in case of tie, pick randomly
  }
} 

head(cbind(usps_test_labels, as.factor(usps_test_pred_Euclidean_cent)))


```

Calculate and plot error rates for the max_k k values

```{r}

library(caret) # for function confusionMatrix

errorRate_Euclidean_cent <- numeric(max_k)
for(i in 1:max_k){
  errorRate_Euclidean_cent [i] <- 1 - confusionMatrix(as.factor(usps_test_pred_Euclidean_cent [, i]), usps_test_labels)$overall["Accuracy"]
}

errorRate_Euclidean_cent

plot(1:max_k, errorRate_Euclidean_cent,
     type = "o", col = "red",
     xlab = "k (number of nearest neighbors used)", ylab = "Misclassification Error Rate",
     ylim = c(0.04, 0.057))

```


Session info

```{r}

R.Version()
R.version
version
sessionInfo()

# rm(list = ls())

```

