---
title: "*K*-Nearest Neighbors (*k*NN) Classification & Cross-Validation"
output: rmarkdown::github_document
date: '2022-08-08'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.retina = 2)
```

## Iris

PROBLEM STATEMENT: for the *k*NN classifier, compare the 5-fold, 10-fold, and leave-one-out cross-validation error rates for *k* = 1, ..., 50 on the classic `Iris` dataset

### Load the Data

```{r}

# import iris data
iris.data <- read.csv(
  "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data", 
  header = FALSE, 
  col.names = c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species"))


```

### Look at the Data

```{r}

head(iris.data)

iris.data[5] <- factor(iris.data[[5]])

str(iris.data)

summary(iris.data)

```

```{r}

pairs(~ ., data = iris.data[-5], 
      col = factor(iris.data[[5]])) 
# this would be clearer if used `Species` instead of `5` in the command

```

### Fit and Validate the *k*NN Model

I used the `caret` package for model fitting and cross-validation

```{r}

# load caret package
library(caret)

```

I tested 5-fold, 10-fold, and leave-one-out cross-validation (CV)

```{r}

# 5-fold, 10-fold, and leave-one-out (LOO) CV
numFolds <- c(5, 10, 150)

# list to store the different fits
fitList <- vector(mode = "list", length = length(numFolds))
names(fitList) <- numFolds

for(i in 1:length(numFolds)){
  trControl <- trainControl(method = "cv",
                            number = numFolds[i])

# vary the number of nearest neighbors k from 1 to 50
set.seed(1250)
fitList[[i]] <- train(Species ~ .,
                        method = "knn",
                        tuneGrid = expand.grid(k = 1:50),
                        trControl = trControl,
                        metric = "Accuracy",
                        data = iris.data)
}
fitList

```

### Analyze Performance Metrics

Plot the results showing the comparison in performance

```{r}

plot(1:50, 1-fitList[[1]]$results$Accuracy,
     type = "o", col = "red",
     xlab = "k (number of nearest neighbors used)", 
     ylab = "Cross-Validation Error Rate", 
     ylim = c(0.01, 0.091))
points(1:50, 1-fitList[[2]]$results$Accuracy, type = "o", col = "blue")
points(1:50, 1-fitList[[3]]$results$Accuracy, type = "o", col = "black")
legend("topleft", legend = c("5-fold CV", "10-fold CV", "LOOCV"),
       col = c("red", "blue", "black"), lty = 1)

```

## USPS digits

PROBLEM STATEMENT: for the *k*NN classifier, compare the test error rates for three different distance metrics (Euclidean, Manhattan, cosine) on the [USPS handwritten zip code digits dataset](https://hastie.su.domains/ElemStatLearn/data.html) 

Load the data

```{r}

library(tidyverse)
usps_train <- read_delim(
  "https://hastie.su.domains/ElemStatLearn/datasets/zip.train.gz", 
  delim = " ", col_names = FALSE)

```

Look at the data.

[Each row is](https://hastie.su.domains/ElemStatLearn/datasets/zip.info.txt) the 256 normalized grayscale values of the 16 x 16 pixel image of a digit.

The first column contains the identity (label) of the digit. The last column is all NA.

```{r}

# str(usps_train)

(usps_train_labels <- usps_train[, 1])
summary(usps_train[, 258])
head(usps_train)

```


```{r}

ls()

```




## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


```{r}



```

